{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of behavioral drift though a _linear_ regression approach \n",
    "\n",
    "__1. Define set of behavioral regressors__\n",
    "\n",
    "    \"Bad Behavior\"\n",
    "    1. Velocity\n",
    "    2. Position on patch  \n",
    "    \n",
    "    \"Uninteresting Behavior\"\n",
    "    3. Reward size \n",
    "    4. Time on patch  \n",
    "    \n",
    "    \"Good Behavior\"\n",
    "    5. Number of Rews  \n",
    "    6. total uL received\n",
    "    7. Time since reward  \n",
    "    8. Reward above expected\n",
    "\n",
    "__2. For every trial, collect data from time after last reward__ \n",
    " - Fit regression between \"time left on patch\" and behavioral regressors\n",
    " - Ensure that fits are roughly equally good using R^2\n",
    "\n",
    "__3. Analyze drift in linear regression model coefficients within session, across sessions__ \n",
    "- Are there discrete or slow changes in model fits that consistently vary across session? \n",
    "- Use Kalman filter to estimate gaussian walk if seems smooth \n",
    "- Assumption: changes in behavior are slow and consistent between reward sizes\n",
    "    - Alternatively, could fit model coefficients per reward size \n",
    "    - But this will not be a good model of single trial lapses in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard stuff\n",
    "import numpy as np  \n",
    "import random\n",
    "import scipy.io as sio  \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns   \n",
    "import pandas as pd    \n",
    "from scipy.stats import zscore\n",
    "# sklearn stuff\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LinearRegression,ElasticNet \n",
    "import scipy.optimize as optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading 79_20200225.mat\n",
      "Finished loading 76_20200305.mat\n",
      "Finished loading 79_20200226.mat\n",
      "Finished loading 76_20200306.mat\n",
      "Finished loading 76_20200307.mat\n",
      "Finished loading 79_20200227.mat\n",
      "Finished loading 76_20200303.mat\n",
      "Finished loading 76_20200302.mat\n",
      "Finished loading 80_20200315.mat\n",
      "Finished loading 80_20200317.mat\n",
      "Finished loading 79_20200304.mat\n",
      "Finished loading 79_20200305.mat\n",
      "Finished loading 80_20200316.mat\n",
      "Finished loading 78_20200312.mat\n",
      "Finished loading 78_20200313.mat\n",
      "Finished loading 78_20200311.mat\n",
      "Finished loading 79_20200302.mat\n",
      "Finished loading 79_20200303.mat\n",
      "Finished loading 78_20200310.mat\n",
      "Finished loading 75_20200313.mat\n",
      "Finished loading 79_20200229.mat\n",
      "Finished loading 76_20200309.mat\n",
      "Finished loading 75_20200315.mat\n",
      "Finished loading 76_20200308.mat\n",
      "Finished loading 79_20200228.mat\n"
     ]
    }
   ],
   "source": [
    "# load all datasets into dictionary\n",
    "mouse_dir = '/Users/joshstern/Documents/UchidaLab_NeuralData/processed_neuropix_data/all_mice'   \n",
    "data = dict()\n",
    "\n",
    "with os.scandir(mouse_dir) as it:\n",
    "    for entry in it:\n",
    "        if not entry.name.startswith('.') and entry.is_file():\n",
    "            data[entry.name[:2] + entry.name[-7:-4]] = sio.loadmat(entry.path) \n",
    "            print(\"Finished loading %s\"%entry.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]  \n",
    "\n",
    "def expo_cdf(dt,N0 = .25,tau = .125):\n",
    "    \"\"\"\n",
    "        Assign reward delivery probabilities according to scaled Expo decay\n",
    "    \"\"\"\n",
    "    x = np.arange(0,50,dt)\n",
    "    cdf = 1 - N0 * np.exp(-tau * x) / tau + N0 / tau  \n",
    "    return np.array(cdf)\n",
    "\n",
    "# superclass \n",
    "class Dataset():\n",
    "    def __init__(self,data,pre_leave_buffer_ms): \n",
    "        self.data = data  \n",
    "        self.pre_leave_buffer_ms = pre_leave_buffer_ms\n",
    "        \n",
    "    def gen_trialedDataset(self,data): \n",
    "        \"\"\"  \n",
    "            Arguments \n",
    "                data: dataset from one session\n",
    "\n",
    "            Returns\n",
    "                X_trials: array of nTrials matrices of taskvars \n",
    "                    Velocity \n",
    "                    Position on patch \n",
    "                    Rewsize \n",
    "                    Time on Patch \n",
    "                    # Rewards \n",
    "                    Total uL \n",
    "                    Time since reward \n",
    "                y_trials: array of nTrials vectors of time to leave from after last reward\n",
    "        \"\"\"   \n",
    "        # timebin\n",
    "        tbin_ms = 1000 * data['velt'][0][1] - data['velt'][0][0]     \n",
    "        dt = tbin_ms / 1000\n",
    "\n",
    "        # session timing timing information\n",
    "        patchCSL = data['patchCSL'] \n",
    "        rewsizes = data['patches'][:,1] % 10\n",
    "        nTrials = patchCSL.shape[0]\n",
    "        patchstop_ms = patchCSL[:,1] * 1000   \n",
    "        patchleave_ms = patchCSL[:,2] * 1000  \n",
    "        prts = patchleave_ms - patchstop_ms \n",
    "        rew_ms = data['rew_ts'] * 1000  \n",
    "        # create index vectors from our timing information vector and labeling settings\n",
    "        patchstop_ix = np.round(patchstop_ms / tbin_ms).astype(int)\n",
    "        patchleave_ix = np.round((patchleave_ms - self.pre_leave_buffer_ms) / tbin_ms).astype(int) \n",
    "        \n",
    "        # Collect behavioral variables\n",
    "        vel_trials = [data['vel'][0][patchstop_ix[iTrial]:patchleave_ix[iTrial]] for iTrial in range(nTrials)]\n",
    "        pos_trials = [data['patch_pos'][0][patchstop_ix[iTrial]:patchleave_ix[iTrial]] for iTrial in range(nTrials)]\n",
    "        t_lens = [len(v) for v in vel_trials]  \n",
    "        round_prts = np.array(t_lens) * tbin_ms / 1000 # prt rounded to nearest dt   \n",
    "        \n",
    "        avg_cdf = expo_cdf(dt)\n",
    "        \n",
    "        # Collect task variables and combine w/ behavioral variables\n",
    "        X_trials = [] # task variables put together in an array \n",
    "        y_trials = [] # label data as on patch or leave patch \n",
    "        last_rew_ix = np.zeros(nTrials,dtype = np.int64) # collect last rew indices to cut down matrices in a second\n",
    "        for iTrial in range(nTrials): \n",
    "            # get the reward timing for this trial (excludes t = 0 reward)\n",
    "            rew_indices = np.round((rew_ms[(rew_ms > patchstop_ms[iTrial]) & (rew_ms < patchleave_ms[iTrial])] - patchstop_ms[iTrial]) / 1000)\n",
    "            last_rew_ix[iTrial] = int(np.round(np.max(np.insert(rew_indices,0,0)) * 1000 / tbin_ms)) \n",
    "            \n",
    "            # time on patch ... and handling of off-by-one errors\n",
    "            if len(np.arange(0,round_prts[iTrial],dt)) == t_lens[iTrial]:\n",
    "                tr_timeOnPatch = np.arange(0,round_prts[iTrial],dt) \n",
    "            elif len(np.arange(0,round_prts[iTrial],dt)) == t_lens[iTrial] + 1:   \n",
    "                if len(np.arange(0,round_prts[iTrial]-dt,dt)) == t_lens[iTrial]: \n",
    "                    tr_timeOnPatch = np.arange(0,round_prts[iTrial]-dt,dt)  \n",
    "                else: \n",
    "                    tr_timeOnPatch = np.arange(0,round_prts[iTrial]-dt/2,dt)  \n",
    "            elif len(np.arange(0,round_prts[iTrial],dt)) == t_lens[iTrial] - 1: \n",
    "                if len(np.arange(0,round_prts[iTrial]+dt,dt)) == t_lens[iTrial]: \n",
    "                    tr_timeOnPatch = np.arange(0,round_prts[iTrial]+dt,dt)  \n",
    "                else: \n",
    "                    tr_timeOnPatch = np.arange(0,round_prts[iTrial]+dt/2,dt)   \n",
    "            else: \n",
    "                print(\"We are off by more than one in indexing somehow\") \n",
    "\n",
    "            # some reward-related signals \n",
    "            tr_rewsize = np.full(len(tr_timeOnPatch),rewsizes[iTrial]) \n",
    "            tr_timesince = tr_timeOnPatch.copy()\n",
    "            tr_rewCount = np.zeros(len(tr_timeOnPatch))\n",
    "            tr_uLtotal = np.zeros(len(tr_timeOnPatch)) \n",
    "            pre_rew_label = np.zeros(t_lens[iTrial])  \n",
    "            for i,rew_sec in enumerate(rew_indices):    \n",
    "                # add to timesince \n",
    "                irew_ix = int(round(rew_sec * 1000 /tbin_ms))  \n",
    "                irew_msec = rew_sec * 1000\n",
    "                try:\n",
    "                    tr_timesince[irew_ix:] = np.arange(0,round_prts[iTrial]-rew_sec,dt)     \n",
    "                except:   \n",
    "                    tr_timesince[irew_ix:] = np.arange(0,round_prts[iTrial]-rew_sec-dt/2,dt)  \n",
    "\n",
    "                # update reward and uL count\n",
    "                tr_rewCount[irew_ix:] = i + 1 \n",
    "                tr_uLtotal[irew_ix:] = (i + 1) * rewsizes[iTrial]  \n",
    "            \n",
    "            tr_rewOverExp = (1 + tr_rewCount) - avg_cdf[:len(tr_rewCount)]\n",
    "            \n",
    "            # now add to data structures\n",
    "            tr_X = np.concatenate((vel_trials[iTrial][:,np.newaxis],\n",
    "                                   pos_trials[iTrial][:,np.newaxis],\n",
    "                                   tr_timeOnPatch[:,np.newaxis], \n",
    "                                   tr_rewsize[:,np.newaxis],\n",
    "                                   tr_timesince[:,np.newaxis],\n",
    "                                   tr_rewCount[:,np.newaxis],\n",
    "                                   tr_uLtotal[:,np.newaxis], \n",
    "                                   tr_rewOverExp[:,np.newaxis]),axis = 1)  \n",
    "            X_trials.append(tr_X)\n",
    "            \n",
    "            tr_y = tr_timeOnPatch[::-1]\n",
    "            y_trials.append(tr_y) \n",
    "        \n",
    "        X_trials_postRew = [X_trials[iTrial][last_rew_ix[iTrial]:,:] for iTrial in range(nTrials)]\n",
    "        y_trials_postRew = [y_trials[iTrial][last_rew_ix[iTrial]:]   for iTrial in range(nTrials)]\n",
    "            \n",
    "        return X_trials,y_trials,X_trials_postRew,y_trials_postRew\n",
    "    \n",
    "    def train_val_split(self,X_trials,y_trials,rewsizes):  \n",
    "        \"\"\" \n",
    "            Function to split training and validation data \n",
    "            On all trials or a subset of trials\n",
    "        \"\"\" \n",
    "        tmp_nTrials = len(y_trials)\n",
    "        \n",
    "        # vector with every index labeled by its trial number\n",
    "        trial_ix = np.array(flatten([[iTrial] * len(y_trials[iTrial]) for iTrial in range(tmp_nTrials)]))\n",
    "        # allocate training and validation trials \n",
    "        train_trials = [] \n",
    "        val_trials = []\n",
    "        for rewsize in [1,2,4]: \n",
    "            iRewsize_trials = np.where(rewsizes == rewsize)[0]  \n",
    "            i_train_trials = random.sample(list(iRewsize_trials),k = int(self.prop_val*len(iRewsize_trials)))  \n",
    "            i_val_trials = np.setdiff1d(iRewsize_trials,i_train_trials)\n",
    "            train_trials.append(i_train_trials) \n",
    "            val_trials.append(i_val_trials.tolist())   \n",
    "        # flatten our lists of lists to make list of trials\n",
    "        train_trials = np.array(flatten(train_trials))\n",
    "        val_trials = np.array(flatten(val_trials)) \n",
    "        train_ix = np.where(np.isin(trial_ix,train_trials)) \n",
    "        val_ix = np.where(np.isin(trial_ix,val_trials)) \n",
    "        # convert to size of train trials\n",
    "        train_trial_ix = trial_ix[train_ix]\n",
    "        \n",
    "        # Now turn separated data into arrays\n",
    "        X_train = np.concatenate([X_trials[trial] for trial in train_trials])\n",
    "        y_train = np.concatenate([y_trials[trial] for trial in train_trials])\n",
    "        X_val = np.concatenate([X_trials[trial] for trial in val_trials]) \n",
    "        y_val = np.concatenate([y_trials[trial] for trial in val_trials])  \n",
    "        \n",
    "        # Now divide training data into k folds, first at the trial level\n",
    "        trial_folds = np.empty(tmp_nTrials) \n",
    "        trial_folds[:] = np.nan \n",
    "        trial_folds[val_trials] = -1 # set aside the validation trials \n",
    "        for shift,iRewsize in enumerate(np.unique(self.rewsizes)):  \n",
    "            iRewsize_train_trials = train_trials[np.where(rewsizes[np.array(train_trials)] == iRewsize)] \n",
    "            iRewsize_folds = np.tile(range(self.k),tmp_nTrials)[:len(iRewsize_train_trials)] \n",
    "            iRewsize_folds = (iRewsize_folds + shift*len(np.unique(rewsizes))) % (self.k) # shift s.t. we don't skimp on the last fold\n",
    "            trial_folds[iRewsize_train_trials] = iRewsize_folds \n",
    "        self.trial_folds = trial_folds\n",
    "\n",
    "        # Now turn our trials into k length list of [(train_index, test_index)]   \n",
    "        kfold_split = []\n",
    "        for test_fold in range(self.k):   \n",
    "            fold_train_trials = np.where((self.trial_folds >= 0) & (self.trial_folds != test_fold))[0]\n",
    "            fold_test_trials = np.where(self.trial_folds == test_fold)[0] \n",
    "            fold_train_ix = np.where(np.isin(train_trial_ix,fold_train_trials))[0] \n",
    "            fold_test_ix = np.where(np.isin(train_trial_ix,fold_test_trials))[0] \n",
    "            kfold_split.append((fold_train_ix,fold_test_ix))  \n",
    "        \n",
    "        return (X_train,y_train,kfold_split,X_val,y_val)\n",
    "    \n",
    "    def fit_model_cv(self,model,X,y,kfold_split,dropout_ix = []): \n",
    "        \"\"\"\n",
    "            Train an Sklearn linear model on trialed X and y data  \n",
    "            \n",
    "            Arguments: \n",
    "                X_trialed (Potentially for some range of trials) \n",
    "                y_trialed (Potentially for some range of trials)\n",
    "            \n",
    "            Returns:  \n",
    "                cv_results_dict\n",
    "        \"\"\"      \n",
    "        coeffs = np.zeros((X.shape[1],self.k))\n",
    "        test_rmse = np.zeros(self.k) \n",
    "        \n",
    "        for f,(train_index, test_index) in enumerate(kfold_split):    \n",
    "            # make new regression pipeline\n",
    "            pipeline = make_pipeline(StandardScaler(),model)\n",
    "            \n",
    "            # split data by fold\n",
    "            fold_X_train, fold_X_test = X[train_index], X[test_index]\n",
    "            fold_y_train, fold_y_test = y[train_index], y[test_index]  \n",
    "\n",
    "            # train models on their different datasets\n",
    "            pipeline = pipeline.fit(fold_X_train, fold_y_train)  \n",
    "            \n",
    "            fold_y_pred = pipeline.predict(fold_X_test) \n",
    "            test_rmse[f] = np.sqrt(metrics.mean_squared_error(fold_y_test, fold_y_pred)) \n",
    "            coeffs[:,f] = pipeline[1].coef_\n",
    "        \n",
    "        cv_fit_results_dict = dict(test_rmse = test_rmse, \n",
    "                                   coeffs = coeffs)\n",
    "        return cv_fit_results_dict \n",
    "    \n",
    "    def fit_elasticNet_cv(self,alpha,l1_ratio): \n",
    "        \"\"\" \n",
    "            Train elasticnet regression to optimize for hyperparameters w.r.t. test set rmse\n",
    "        \"\"\"   \n",
    "        elasticnet = ElasticNet(alpha = alpha,l1_ratio = l1_ratio) \n",
    "        results_dict = session_data.fit_model_cv(elasticnet,self.X_train,self.y_train,self.kfold_split) \n",
    "        \n",
    "        return np.mean(results_dict['test_rmse']) \n",
    "    \n",
    "    def dropout_test(self,X,y,kfold_split):  \n",
    "        \"\"\" \n",
    "            Train a logistic regression model on trialed X and y data to analyze variables that improve ll\n",
    "        \n",
    "            \n",
    "            Arguments:  \n",
    "                X_trialed (Potentially for some range of trials) \n",
    "                y_trialed (Potentially for some range of trials) \n",
    "                kfold_split: kfold xval index tuples \n",
    "            \n",
    "            Return: \n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplemented \n",
    "\n",
    "class MouseDataset(Dataset): \n",
    "    def __init__(self,data,mouse,pre_leave_buffer_ms): \n",
    "        self.pre_leave_buffer_ms = pre_leave_buffer_ms\n",
    "        \n",
    "        mouse_days = [day for day in list(data.keys()) if mouse in day] # find all mouse days \n",
    "        sorted_mouse_days = list(map(str,np.sort([int(day) for day in mouse_days]))) \n",
    "        self.data = {mouse_day: data[mouse_day] for mouse_day in sorted_mouse_days} # subselect mouse data \n",
    "        tbin_ms = self.data[mouse_days[0]]['velt'][0][1] - self.data[mouse_days[0]]['velt'][0][0]\n",
    "        \n",
    "class SessionDataset(Dataset): \n",
    "    def __init__(self,data,day,pre_leave_buffer_ms,k = 5,prop_val = 0.9): \n",
    "        self.data = data[day] # subselect session data     \n",
    "        self.rewsizes = self.data['patches'][:,1] % 10 \n",
    "        self.nTrials = len(self.rewsizes) \n",
    "        self.k = k \n",
    "        self.prop_val = prop_val \n",
    "        self.pre_leave_buffer_ms = pre_leave_buffer_ms\n",
    "        \n",
    "        # make regression dataset \n",
    "        (self.X_trials,\n",
    "         self.y_trials,\n",
    "         self.X_trials_postRew,\n",
    "         self.y_trials_postRew) = self.gen_trialedDataset(self.data)    \n",
    "        \n",
    "        # split data into train and validation sets, training set into k folds\n",
    "        (self.X_train,self.y_train,\n",
    "         self.kfold_split,\n",
    "         self.X_val,self.y_val) = self.train_val_split(self.X_trials_postRew,self.y_trials_postRew,self.rewsizes)\n",
    "        \n",
    "        self.alpha_optimized,self.l1_ratio_optimized = optimize_elasticNet_params(self)\n",
    "        \n",
    "class SubSessionDataset(Dataset): \n",
    "    def __init__(self,data,day,pre_leave_buffer_ms,window_len,window_stride,k = 5,prop_val = .9): \n",
    "        self.data = data[day] # subselect session data \n",
    "        self.rewsizes = self.data['patches'][:,1] % 10 \n",
    "        self.nTrials = len(self.rewsizes) \n",
    "        self.k = k \n",
    "        self.prop_val = prop_val \n",
    "        self.pre_leave_buffer_ms = pre_leave_buffer_ms\n",
    "        \n",
    "        # make classification dataset\n",
    "        (self.X_trials,\n",
    "         self.y_trials,\n",
    "         self.X_trials_postRew,\n",
    "         self.y_trials_postRew) = self.gen_trialedDataset(self.data)     \n",
    "        \n",
    "        # split data into kfold train and val sets over windows\n",
    "        nTrials = len(self.y_trials)\n",
    "        # now make training and validation datasets per window of session  \n",
    "        self.window_starts = np.arange(0,nTrials - window_len,window_stride)   \n",
    "        self.window_ends = self.window_starts + window_len\n",
    "        self.X_train = [] \n",
    "        self.y_train = [] \n",
    "        self.kfold_split = [] \n",
    "        self.X_val = [] \n",
    "        self.y_val = []  \n",
    "        for (start,end) in zip(self.window_starts,self.window_ends): \n",
    "            trials = np.arange(start,end-1)    \n",
    "            (X_train,y_train,\n",
    "             kfold_split,\n",
    "             X_val,y_val) = self.train_val_split(np.array(self.X_trials_postRew)[trials],\n",
    "                                                 np.array(self.y_trials_postRew)[trials],\n",
    "                                                 self.rewsizes[trials]) \n",
    "            self.X_train.append(X_train) \n",
    "            self.y_train.append(y_train) \n",
    "            self.kfold_split.append(kfold_split) \n",
    "            self.X_val.append(X_val) \n",
    "            self.y_val.append(y_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticnet hyperparameter optimization methods \n",
    "\n",
    "def fit_elasticNet_cv(alpha_l1ratio,session_data): \n",
    "        \"\"\" \n",
    "            Train elasticnet regression to optimize for hyperparameters w.r.t. test set rmse\n",
    "        \"\"\"    \n",
    "        elasticnet = ElasticNet(alpha = alpha_l1ratio[0],l1_ratio = alpha_l1ratio[1]) \n",
    "        results_dict = session_data.fit_model_cv(elasticnet,session_data.X_train,session_data.y_train,session_data.kfold_split) \n",
    "        \n",
    "        return np.mean(results_dict['test_rmse'])   \n",
    "    \n",
    "def optimize_elasticNet_params(session_data): \n",
    "    \"\"\" \n",
    "        Scipy optimize.minimize call\n",
    "    \"\"\"\n",
    "    x0 = (.5,.5) \n",
    "    bnds = [(.01,.99),(0.01,.99)] \n",
    "    alpha,l1_ratio = optimize.minimize(fit_elasticNet_cv,x0,(session_data),bounds = bnds)[\"x\"]\n",
    "    return alpha,l1_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions \n",
    "def vis_coeffs(coeffs,metrics_dict,session_title): \n",
    "    \"\"\" \n",
    "        Visualize Coefficients\n",
    "    \"\"\"  \n",
    "    AP = np.mean(metrics_dict['ap']) \n",
    "    auROC = np.mean(metrics_dict['auc'])\n",
    "    plt.figure()   \n",
    "    black = [.4,.4,.4] \n",
    "    red = [0.8392156862745098, 0.15294117647058825, 0.1568627450980392]\n",
    "    blue = [0.12156862745098039, 0.4666666666666667, 0.7058823529411765]\n",
    "    colors = sns.color_palette([black,black,red,red,blue,blue,blue]) \n",
    "    sns.barplot(data = coeffs,palette = colors) \n",
    "    plt.xticks(np.arange(7),[\"Velocity\",\"Position\",\"Time on patch\",\"Reward size\",\"Time Since Reward\",\"Reward Count\",\"Total uL\"],rotation = \"vertical\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_leave_buffer = 500\n",
    "session_data = SessionDataset(data,\"78310\",pre_leave_buffer)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unregularized Test RMSE: 6.813888231891445\n",
      "ElasticNet Test RMSE: 6.6776047425128455\n"
     ]
    }
   ],
   "source": [
    "elasticnet = ElasticNet(alpha = session_data.alpha_optimized,l1_ratio = session_data.l1_ratio_optimized) \n",
    "linreg = LinearRegression() \n",
    "\n",
    "linreg_results_dict = session_data.fit_model_cv(linreg,session_data.X_train,session_data.y_train,session_data.kfold_split) \n",
    "elastic_results_dict = session_data.fit_model_cv(elasticnet,session_data.X_train,session_data.y_train,session_data.kfold_split) \n",
    "print(\"Unregularized Test RMSE:\",np.mean(linreg_results_dict['test_rmse']))\n",
    "print(\"ElasticNet Test RMSE:\",np.mean(elastic_results_dict['test_rmse']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x391c370d0>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(results_dict['coeffs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51643061, 0.54483701, 0.6434332 , 0.51403947, 0.48910329])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict['test_rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expo_cdf(dt,N0 = .25,tau = .125):\n",
    "    \"\"\"\n",
    "        Assign reward delivery probabilities according to scaled Expo decay\n",
    "    \"\"\"\n",
    "    x = np.arange(0,50,dt)\n",
    "    cdf = 1 - N0 * np.exp(-tau * x) / tau + N0 / tau  \n",
    "    return np.array(cdf)\n",
    "\n",
    "plt.plot( - expo_cdf(.25,.125,.05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
